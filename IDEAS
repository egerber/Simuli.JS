So far a Model was implemented to learn the mapping from input(t) -> input(t+1). This implementation should be altered in order to perform a more general mapping from input(t) -> output(t) where output(t) can be input(t+1) as well. This would create the opportunity to easily build hierarchies of models. The goal is to arrive at a model that makes predictions by means of a very limited set of patterns. These patterns are provided by lower level models. 
Example: The input is of length 50. The goal is to predict the input of the next timestep. So far, the model can learn to build the correct synapses between input(t) and input(t+1) in order to find a stable transition and be able to make a solid prediction. However this requires a lot of synapses (e.g. 10 synapses per input item, 50 in this example). Furthermore, the learned predictions will have no value when a different type of input pattern is applied. Also when we look at cellular automata, this approach seems not smart, since the amount of rules for the pattern is very limited. 
Ideally the model should come up with more "compressed" patterns, that is less synapses and more globally used patterns. Optimally, these patterns should hold some value when a new input pattern is applied.
This could be achieved by building up a hierarchy of models. For the example with 50 inputs, this could look like this:
The first layer creates k (e.g. 10) m:n models (e.g. 10:3, inputs to outputs). The selection of these models (or patterns) is according to how useful they are in making correct predictions. The patterns then would be used by layer 2, which optmizes as well as the selection of patterns to be used for the prediction of a specific input item, as well as which inputs from t+1 are passed into a pattern and which corresponding input item is connected to it. The patterns on layer 1 would be optimized based on how well the performance on layer 2 is. 
-> somehow, layer 2 must make prediction by EITHER choosing from a limited set of patterns OR choosing a limited number of patterns from a large set (including specific synapses)



The current implementation defines 0 and 1 as two different states, without attributing any meaning to them. This makes it hard to encode patterns. It is impossible to define a unit that receives multiple input and outputs whether one specfic input configuration applied (e.g. 0001101). This is only possible if we define the output state 1 as a boolean indicator for the existence of a pattern. 
-> I could implement a unit that is given k random inputs and that learns to output 1 for the most frequent input configuration that is receives. The following layer could use this information in order to make valid predictions. The predictions could again refer to some pattern. e.g. pattern_1(t)->pattern_25(t+1)

-> How could fuzziness be introduced to this approach without introducing probabilities or specifying the minimal number of matching inputs for a pattern (e.g output 1 if at least 4 out of 5 inputs match)
	-> OR method: fire if the inputs match to any of multiple configurations



How does the neural information processing work?
-> The dendrites of a neuron receive inputs from nearby neurons. This input is binary and either states activation or no activation. If multiple dendrites receive activation at about the same time, the receiver-neuron sends activation to other neurons. The connection from the sending neurons will be strengthened.
Thereby the output from a neuron indicates whether it has received enough input at the time.



A pattern as a specific combination of values has no meaning in itself. It is easy to detect frequent combination of values in a stream of input. But it is much harder to find frequent transitions of patterns 
Now the problem that poses is the following: How do I optimize at the same time the definition of useful patterns as well as the selection from these patterns in order to make strong predictions?

-unit should set all internal_states and get all internal_states by generic method
-unit should should be able to add connections and choose among a certain set of surrounded units (e.g. local relationship).
-units should only be varying with regard to their input(s) and output, how the input to output behavior is defined, and how updates are performed (internal parameters are changed and reconnections are formed)

--- 

-think of a different encoding for logs
e.g. for events it is better in the following manner: {event_name:event1, id:12, timesteps:[10,11,12,13,14,etc]} instead of {t:10,id:12,event_name:event1}

-look for solid visualualization package (filter functions in the manner of simulinks analyzer, zooming etc.) that can be extended with individual packages. 
(OR at least some package with functionality for filtering data)

----

Give unit not only access to local state object but also to global state reference
-> approximate behavior of ressource access

dirtybit could be used to indicate newly created cells as well as well as newly formed connections
00 (0) - no changes 
01 (1) - connection changed
10 (2) - cell was newly added
11 (3) - cell was added and new connections applied

---------------

delay lines
- e.g. delayed feedback
-> general: make it possible to define in which order the nodes are calculated and the delay of communication between nodes
	-delay 0: same timestep calculation
	-delay k: value is passed over in next timestep (for k>=1)
	-> allows to built graph of the whole network and bring the network into a different shape
	connection(src,target,mapping,delay=0)
---------------

Build units from collection of pre-existing neurons
- e.g. neuron with 10 synapses becomes one unit (synapses are input units, neuron is output unit)

----

Abandon local references:
- everything is referenced by index -> index is resolved by Factory method
- create and delete 
- get inputs() resolves index references to objects
- connections are saved as graph by public object (makes it easier and more efficient to find links e.g. triggering all on_deletes)
- possibly: keep objects as a method to setup a network, internally convert everything to arrays and array methods

- can I create a new class by means of a specification?


Define
- generate_feedback(input,output, state)
- forward_feedback(input,output, state, feedback_given)
Feedback is sent after all feedback values where received
 -> how can this be enforced?



 UNIT:
 get count_inputs
 - input_slots (e.g. 5)
 - dim_output (e.g. 0, [0], [1,1])

allow to define feedback and feedforward connections explicitly
- connect(type: feedback | feedforward | bidirectional)
- feedforward/feedback lines with delay

compute_feedback (regardless of feedback_given)
tick() does not trigger feedback sending implicitly
	- tick() of system invokes send feedback calls in correct order

Spatial Connections (e.g 2D Input Array)

Network
- specify how the input of a network is transformed into the activation of the Input Group
- specify how the output of the network is computed by means of the network activation

- optional: transform_input(inputs) -> returns array which is passed to inputs
- optional: compute_output(groups) referencing groups

set_feedback_component
set feedback

feedback_inputs (indicate from where to receive feedback)
feedback_outputs (indicate where to send feedback signal to)

compute_feedback ~ output,state

apply_feedback ~ output, state, feedback

